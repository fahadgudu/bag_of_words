
<!DOCTYPE html
  PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   <!--
This HTML was auto-generated from MATLAB code.
To make changes, update the MATLAB code and republish this document.
      --><title>svm_classify</title><meta name="generator" content="MATLAB 8.5"><link rel="schema.DC" href="http://purl.org/dc/elements/1.1/"><meta name="DC.date" content="2016-01-20"><meta name="DC.source" content="svm_classify.m"><style type="text/css">
html,body,div,span,applet,object,iframe,h1,h2,h3,h4,h5,h6,p,blockquote,pre,a,abbr,acronym,address,big,cite,code,del,dfn,em,font,img,ins,kbd,q,s,samp,small,strike,strong,sub,sup,tt,var,b,u,i,center,dl,dt,dd,ol,ul,li,fieldset,form,label,legend,table,caption,tbody,tfoot,thead,tr,th,td{margin:0;padding:0;border:0;outline:0;font-size:100%;vertical-align:baseline;background:transparent}body{line-height:1}ol,ul{list-style:none}blockquote,q{quotes:none}blockquote:before,blockquote:after,q:before,q:after{content:'';content:none}:focus{outine:0}ins{text-decoration:none}del{text-decoration:line-through}table{border-collapse:collapse;border-spacing:0}

html { min-height:100%; margin-bottom:1px; }
html body { height:100%; margin:0px; font-family:Arial, Helvetica, sans-serif; font-size:10px; color:#000; line-height:140%; background:#fff none; overflow-y:scroll; }
html body td { vertical-align:top; text-align:left; }

h1 { padding:0px; margin:0px 0px 25px; font-family:Arial, Helvetica, sans-serif; font-size:1.5em; color:#d55000; line-height:100%; font-weight:normal; }
h2 { padding:0px; margin:0px 0px 8px; font-family:Arial, Helvetica, sans-serif; font-size:1.2em; color:#000; font-weight:bold; line-height:140%; border-bottom:1px solid #d6d4d4; display:block; }
h3 { padding:0px; margin:0px 0px 5px; font-family:Arial, Helvetica, sans-serif; font-size:1.1em; color:#000; font-weight:bold; line-height:140%; }

a { color:#005fce; text-decoration:none; }
a:hover { color:#005fce; text-decoration:underline; }
a:visited { color:#004aa0; text-decoration:none; }

p { padding:0px; margin:0px 0px 20px; }
img { padding:0px; margin:0px 0px 20px; border:none; }
p img, pre img, tt img, li img, h1 img, h2 img { margin-bottom:0px; } 

ul { padding:0px; margin:0px 0px 20px 23px; list-style:square; }
ul li { padding:0px; margin:0px 0px 7px 0px; }
ul li ul { padding:5px 0px 0px; margin:0px 0px 7px 23px; }
ul li ol li { list-style:decimal; }
ol { padding:0px; margin:0px 0px 20px 0px; list-style:decimal; }
ol li { padding:0px; margin:0px 0px 7px 23px; list-style-type:decimal; }
ol li ol { padding:5px 0px 0px; margin:0px 0px 7px 0px; }
ol li ol li { list-style-type:lower-alpha; }
ol li ul { padding-top:7px; }
ol li ul li { list-style:square; }

.content { font-size:1.2em; line-height:140%; padding: 20px; }

pre, code { font-size:12px; }
tt { font-size: 1.2em; }
pre { margin:0px 0px 20px; }
pre.codeinput { padding:10px; border:1px solid #d3d3d3; background:#f7f7f7; }
pre.codeoutput { padding:10px 11px; margin:0px 0px 20px; color:#4c4c4c; }
pre.error { color:red; }

@media print { pre.codeinput, pre.codeoutput { word-wrap:break-word; width:100%; } }

span.keyword { color:#0000FF }
span.comment { color:#228B22 }
span.string { color:#A020F0 }
span.untermstring { color:#B20000 }
span.syscmd { color:#B28C00 }

.footer { width:auto; padding:10px 0px; margin:25px 0px 0px; border-top:1px dotted #878787; font-size:0.8em; line-height:140%; font-style:italic; color:#878787; text-align:left; float:none; }
.footer p { margin:0px; }
.footer a { color:#878787; }
.footer a:hover { color:#878787; text-decoration:underline; }
.footer a:visited { color:#878787; }

table th { padding:7px 5px; text-align:left; vertical-align:middle; border: 1px solid #d6d4d4; font-weight:bold; }
table td { padding:7px 5px; text-align:left; vertical-align:top; border:1px solid #d6d4d4; }





  </style></head><body><div class="content"><pre class="codeinput"><span class="comment">% Starter code prepared by James Hays for CS 143, Brown University</span>

<span class="comment">%This function will train a linear SVM for every category (i.e. one vs all)</span>
<span class="comment">%and then use the learned linear classifiers to predict the category of</span>
<span class="comment">%every test image. Every test feature will be evaluated with all 15 SVMs</span>
<span class="comment">%and the most confident SVM will "win". Confidence, or distance from the</span>
<span class="comment">%margin, is W*X + B where '*' is the inner product or dot product and W and</span>
<span class="comment">%B are the learned hyperplane parameters.</span>

<span class="keyword">function</span> predicted_categories = svm_classify(train_image_feats, train_labels, test_image_feats)
<span class="comment">% image_feats is an N x d matrix, where d is the dimensionality of the</span>
<span class="comment">%  feature representation.</span>
<span class="comment">% train_labels is an N x 1 cell array, where each entry is a string</span>
<span class="comment">%  indicating the ground truth category for each training image.</span>
<span class="comment">% test_image_feats is an M x d matrix, where d is the dimensionality of the</span>
<span class="comment">%  feature representation. You can assume M = N unless you've modified the</span>
<span class="comment">%  starter code.</span>
<span class="comment">% predicted_categories is an M x 1 cell array, where each entry is a string</span>
<span class="comment">%  indicating the predicted category for each test image.</span>

<span class="comment">%{
</span><span class="comment">Useful functions:
</span><span class="comment"> matching_indices = strcmp(string, cell_array_of_strings)
</span><span class="comment">
</span><span class="comment">  This can tell you which indices in train_labels match a particular
</span><span class="comment">  category. This is useful for creating the binary labels for each SVM
</span><span class="comment">  training task.
</span><span class="comment">
</span><span class="comment">[W B] = vl_svmtrain(features, labels, LAMBDA)
</span><span class="comment">  http://www.vlfeat.org/matlab/vl_svmtrain.html
</span><span class="comment">
</span><span class="comment">  This function trains linear svms based on training examples, binary
</span><span class="comment">  labels (-1 or 1), and LAMBDA which regularizes the linear classifier
</span><span class="comment">  by encouraging W to be of small magnitude. LAMBDA is a very important
</span><span class="comment">  parameter! You might need to experiment with a wide range of values for
</span><span class="comment">  LAMBDA, e.g. 0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10.
</span><span class="comment">
</span><span class="comment">  Matlab has a built in SVM, see 'help svmtrain', which is more general,
</span><span class="comment">  but it obfuscates the learned SVM parameters in the case of the linear
</span><span class="comment">  model. This makes it hard to compute "confidences" which are needed for
</span><span class="comment">  one-vs-all classification.
</span><span class="comment">
</span><span class="comment">%}
</span>
<span class="comment">%unique() is used to get the category list from the observed training</span>
<span class="comment">%category list. 'categories' will not be in the same order as in proj3.m,</span>
<span class="comment">%because unique() sorts them. This shouldn't really matter, though.</span>
unique_label = unique(train_labels);
<span class="keyword">for</span> i = 1:length(unique_label);
    class_find = strcmp(train_labels, unique_label{i});
    labels(~class_find) = -1;
    labels(class_find) = 1;
    [W,B] = vl_svmtrain(train_image_feats', labels, 0.0005);
    svn(i,:) = W'* test_image_feats' + B;
<span class="keyword">end</span>
[~, index] = max(svn);
predicted_categories = unique_label(index);
</pre><pre class="codeoutput error">Error using svm_classify (line 48)
Not enough input arguments.
</pre><p class="footer"><br><a href="http://www.mathworks.com/products/matlab/">Published with MATLAB&reg; R2015a</a><br></p></div><!--
##### SOURCE BEGIN #####
% Starter code prepared by James Hays for CS 143, Brown University

%This function will train a linear SVM for every category (i.e. one vs all)
%and then use the learned linear classifiers to predict the category of
%every test image. Every test feature will be evaluated with all 15 SVMs
%and the most confident SVM will "win". Confidence, or distance from the
%margin, is W*X + B where '*' is the inner product or dot product and W and
%B are the learned hyperplane parameters.

function predicted_categories = svm_classify(train_image_feats, train_labels, test_image_feats)
% image_feats is an N x d matrix, where d is the dimensionality of the
%  feature representation.
% train_labels is an N x 1 cell array, where each entry is a string
%  indicating the ground truth category for each training image.
% test_image_feats is an M x d matrix, where d is the dimensionality of the
%  feature representation. You can assume M = N unless you've modified the
%  starter code.
% predicted_categories is an M x 1 cell array, where each entry is a string
%  indicating the predicted category for each test image.

%{
Useful functions:
 matching_indices = strcmp(string, cell_array_of_strings)
 
  This can tell you which indices in train_labels match a particular
  category. This is useful for creating the binary labels for each SVM
  training task.

[W B] = vl_svmtrain(features, labels, LAMBDA)
  http://www.vlfeat.org/matlab/vl_svmtrain.html

  This function trains linear svms based on training examples, binary
  labels (-1 or 1), and LAMBDA which regularizes the linear classifier
  by encouraging W to be of small magnitude. LAMBDA is a very important
  parameter! You might need to experiment with a wide range of values for
  LAMBDA, e.g. 0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10.

  Matlab has a built in SVM, see 'help svmtrain', which is more general,
  but it obfuscates the learned SVM parameters in the case of the linear
  model. This makes it hard to compute "confidences" which are needed for
  one-vs-all classification.

%}

%unique() is used to get the category list from the observed training
%category list. 'categories' will not be in the same order as in proj3.m,
%because unique() sorts them. This shouldn't really matter, though.
unique_label = unique(train_labels);
for i = 1:length(unique_label);
    class_find = strcmp(train_labels, unique_label{i});
    labels(~class_find) = -1;
    labels(class_find) = 1;
    [W,B] = vl_svmtrain(train_image_feats', labels, 0.0005);
    svn(i,:) = W'* test_image_feats' + B; 
end
[~, index] = max(svn);
predicted_categories = unique_label(index);


##### SOURCE END #####
--></body></html>